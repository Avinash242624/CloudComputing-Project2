{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKFvWn9+4KmetelcmEa7e9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avinash242624/project2/blob/main/Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI4zCtlFptNF",
        "outputId": "c693f252-5ff4-41b3-b041-72f7f5a6a95b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.35.76-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.36.0,>=1.35.76 (from boto3)\n",
            "  Downloading botocore-1.35.76-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.76->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.76->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.76->boto3) (1.16.0)\n",
            "Downloading boto3-1.35.76-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.76-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.35.76 botocore-1.35.76 jmespath-1.0.1 s3transfer-0.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "def download_and_convert_ndjson_to_csv(bucket_name, local_folder):\n",
        "    # Create an S3 client with anonymous access\n",
        "    s3_client = boto3.client('s3', region_name='us-east-1', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "    # List objects in the S3 bucket\n",
        "    response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
        "\n",
        "    if 'Contents' not in response:\n",
        "        print(\"No files found in the bucket.\")\n",
        "        return\n",
        "\n",
        "    # Process each JSON file directly from S3 and convert to CSV\n",
        "    for obj in response['Contents']:\n",
        "        file_key = obj['Key']\n",
        "\n",
        "        # Check if the file is a JSON file\n",
        "        if not file_key.endswith('.json'):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {file_key}...\")\n",
        "\n",
        "        # Download the NDJSON content directly from S3\n",
        "        json_content = s3_client.get_object(Bucket=bucket_name, Key=file_key)['Body'].read().decode('utf-8')\n",
        "\n",
        "        # Convert NDJSON content to CSV\n",
        "        convert_ndjson_content_to_csv(json_content, file_key, local_folder)\n",
        "\n",
        "def convert_ndjson_content_to_csv(json_content, file_key, local_folder):\n",
        "    \"\"\"Convert NDJSON content to CSV format and save it.\"\"\"\n",
        "    try:\n",
        "        # Split content by lines (each line is a JSON object)\n",
        "        lines = json_content.splitlines()\n",
        "\n",
        "        # Convert each JSON line to a dictionary and store in a list\n",
        "        data = [json.loads(line) for line in lines if line.strip()]\n",
        "\n",
        "        # Convert the list of dictionaries to a DataFrame\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Create the local CSV file path\n",
        "        csv_file_path = os.path.join(local_folder, file_key.replace('.json', '.csv'))\n",
        "        os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
        "\n",
        "        # Save the DataFrame to a CSV file\n",
        "        df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
        "        print(f\"Converted to CSV: {csv_file_path}\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decoding error in {file_key}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting {file_key} to CSV: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    bucket_name = 'helpful-sentences-from-reviews'\n",
        "    local_folder = './customer_review_dataset'\n",
        "\n",
        "    if not os.path.exists(local_folder):\n",
        "        os.makedirs(local_folder)\n",
        "\n",
        "    download_and_convert_ndjson_to_csv(bucket_name, local_folder)\n",
        "    print(\"All files converted to CSV and saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPxvOzi3qAQh",
        "outputId": "7c0b5f67-fe5f-41c5-d4c9-8598386cdf0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing test.json...\n",
            "Converted to CSV: ./customer_review_dataset/test.csv\n",
            "Processing train.json...\n",
            "Converted to CSV: ./customer_review_dataset/train.csv\n",
            "All files converted to CSV and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date\n",
        "import os\n",
        "\n",
        "# Verify file paths\n",
        "sales_data_path = \"/content/input/train.csv\"\n",
        "reviews_data_path = \"/content/input/Online Retail.csv\"\n",
        "\n",
        "# Define output paths\n",
        "output_sales_data_path = \"/content/output/sales_data_clean.csv\"\n",
        "output_reviews_data_path = \"/content/output/review_data_clean.csv\"\n",
        "\n",
        "# Check if input files exist\n",
        "if not os.path.exists(sales_data_path):\n",
        "    print(f\"Error: Sales data file not found at {sales_data_path}\")\n",
        "    exit(1)\n",
        "\n",
        "if not os.path.exists(reviews_data_path):\n",
        "    print(f\"Error: Reviews data file not found at {reviews_data_path}\")\n",
        "    exit(1)\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Retail Sales and Demand Forecasting\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the datasets from CSV files\n",
        "sales_df = spark.read.csv(sales_data_path, header=True, inferSchema=True)\n",
        "reviews_df = spark.read.csv(reviews_data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Print the schema to understand data types\n",
        "print(\"Sales Data Schema:\")\n",
        "sales_df.printSchema()\n",
        "print(\"Reviews Data Schema:\")\n",
        "reviews_df.printSchema()\n",
        "\n",
        "# Count the number of missing values in each column\n",
        "print(\"Missing Values in Sales Data:\")\n",
        "for column in sales_df.columns:\n",
        "    missing_count = sales_df.filter(sales_df[column].isNull()).count()\n",
        "    if missing_count > 0:\n",
        "        print(f\"{column}: {missing_count}\")\n",
        "\n",
        "print(\"Missing Values in Reviews Data:\")\n",
        "for column in reviews_df.columns:\n",
        "    missing_count = reviews_df.filter(reviews_df[column].isNull()).count()\n",
        "    if missing_count > 0:\n",
        "        print(f\"{column}: {missing_count}\")\n",
        "\n",
        "# Data Cleansing\n",
        "# Drop duplicates\n",
        "sales_df = sales_df.dropDuplicates()\n",
        "reviews_df = reviews_df.dropDuplicates()\n",
        "\n",
        "# Handle missing values and parse dates if 'date' column is present in sales data\n",
        "if 'date' in sales_df.columns:\n",
        "    sales_df = sales_df.na.drop(subset=[\"date\"])  # Drops rows where 'date' is null\n",
        "    sales_df = sales_df.withColumn(\"date\", to_date(sales_df[\"date\"], \"yyyy-MM-dd\"))  # Convert date format\n",
        "else:\n",
        "    print(\"No 'date' column found in sales data\")\n",
        "\n",
        "# Drop rows with missing review data\n",
        "reviews_df = reviews_df.na.drop()\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(output_sales_data_path), exist_ok=True)\n",
        "os.makedirs(os.path.dirname(output_reviews_data_path), exist_ok=True)\n",
        "\n",
        "# Output the cleaned data to CSV files\n",
        "sales_df.write.csv(output_sales_data_path, header=True, mode=\"overwrite\")\n",
        "reviews_df.write.csv(output_reviews_data_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Print confirmation of saved files\n",
        "print(f\"Cleaned sales data saved to {output_sales_data_path}\")\n",
        "print(f\"Cleaned reviews data saved to {output_reviews_data_path}\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFt0WsguqU7O",
        "outputId": "987465fd-2912-4826-9e0d-2ce719ca5bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales Data Schema:\n",
            "root\n",
            " |-- asin: string (nullable = true)\n",
            " |-- sentence: string (nullable = true)\n",
            " |-- helpful: string (nullable = true)\n",
            " |-- main_image_url: string (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            "\n",
            "Reviews Data Schema:\n",
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "Missing Values in Sales Data:\n",
            "main_image_url: 3\n",
            "product_title: 4\n",
            "Missing Values in Reviews Data:\n",
            "Description: 1454\n",
            "CustomerID: 135080\n",
            "No 'date' column found in sales data\n",
            "Cleaned sales data saved to /content/output/sales_data_clean.csv\n",
            "Cleaned reviews data saved to /content/output/review_data_clean.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, avg, month, to_date\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Initialize SparkSession with legacy time parser policy\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sales Data Aggregation and Feature Engineering\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define file paths\n",
        "online_retail_data_path = \"/content/input/Online Retail.csv\"\n",
        "\n",
        "# Define output path\n",
        "output_base_path = \"/content/output/task2\"\n",
        "\n",
        "# Resolve path conflicts\n",
        "if os.path.exists(output_base_path) and not os.path.isdir(output_base_path):\n",
        "    print(f\"Conflict detected: {output_base_path} is a file. Deleting it.\")\n",
        "    os.remove(output_base_path)\n",
        "if os.path.exists(output_base_path):\n",
        "    shutil.rmtree(output_base_path)\n",
        "os.makedirs(output_base_path)\n",
        "\n",
        "# Load the dataset\n",
        "online_retail_df = spark.read.csv(online_retail_data_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert date column\n",
        "online_retail_df = online_retail_df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/dd/yyyy\"))\n",
        "\n",
        "# Sales Aggregation: Total sales per product per month\n",
        "output_total_sales = os.path.join(output_base_path, \"total_sales_per_product_per_month\")\n",
        "os.makedirs(output_total_sales, exist_ok=True)\n",
        "total_sales_per_product_per_month = online_retail_df.groupBy(\n",
        "    \"StockCode\", month(\"InvoiceDate\").alias(\"month\")\n",
        ").agg(sum(\"Quantity\").alias(\"total_sales\"))\n",
        "total_sales_per_product_per_month.write.csv(output_total_sales, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Sales Aggregation: Average revenue per customer\n",
        "output_avg_revenue = os.path.join(output_base_path, \"average_revenue_per_customer\")\n",
        "os.makedirs(output_avg_revenue, exist_ok=True)\n",
        "average_revenue_per_customer = online_retail_df.groupBy(\"CustomerID\").agg(avg(\"UnitPrice\").alias(\"average_revenue\"))\n",
        "average_revenue_per_customer.write.csv(output_avg_revenue, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Seasonal patterns for top-selling products\n",
        "output_top_selling = os.path.join(output_base_path, \"top_selling_products\")\n",
        "os.makedirs(output_top_selling, exist_ok=True)\n",
        "top_selling_products = online_retail_df.groupBy(\"StockCode\").agg(sum(\"Quantity\").alias(\"total_sales\")).orderBy(\n",
        "    col(\"total_sales\").desc()\n",
        ")\n",
        "top_selling_products.write.csv(output_top_selling, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Customer lifetime value\n",
        "output_lifetime_value = os.path.join(output_base_path, \"customer_lifetime_value\")\n",
        "os.makedirs(output_lifetime_value, exist_ok=True)\n",
        "customer_lifetime_value = online_retail_df.groupBy(\"CustomerID\").agg(sum(\"Quantity\").alias(\"lifetime_value\"))\n",
        "customer_lifetime_value.write.csv(output_lifetime_value, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Product popularity score\n",
        "output_popularity = os.path.join(output_base_path, \"product_popularity_score\")\n",
        "os.makedirs(output_popularity, exist_ok=True)\n",
        "product_popularity_score = online_retail_df.groupBy(\"StockCode\").count().withColumnRenamed(\"count\", \"popularity_score\")\n",
        "product_popularity_score.write.csv(output_popularity, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Seasonal trends\n",
        "output_seasonal_trends = os.path.join(output_base_path, \"seasonal_trends\")\n",
        "os.makedirs(output_seasonal_trends, exist_ok=True)\n",
        "seasonal_trends = online_retail_df.groupBy(\n",
        "    \"StockCode\", month(\"InvoiceDate\").alias(\"month\")\n",
        ").agg(sum(\"Quantity\").alias(\"monthly_sales\"))\n",
        "seasonal_trends.write.csv(output_seasonal_trends, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLH-Fh8dqgGI",
        "outputId": "1621e840-774c-49b1-9b2d-b82218f6e640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conflict detected: /content/output/task2 is a file. Deleting it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col, year, month, sum\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml import Pipeline\n",
        "import os\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Retail Sales Demand Forecasting\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load Data\n",
        "data = spark.read.csv('/content/input/Online Retail.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Data Preprocessing\n",
        "data = data.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
        "data = data.dropna(how='any')\n",
        "\n",
        "# Feature Engineering\n",
        "data = data.withColumn('Year', year(col('InvoiceDate')))\n",
        "data = data.withColumn('Month', month(col('InvoiceDate')))\n",
        "data = data.groupBy('Year', 'Month', 'StockCode').agg(sum('Quantity').alias('TotalQuantity'))\n",
        "\n",
        "# Reduce Dataset Size for Testing (Optional)\n",
        "data = data.limit(10000)\n",
        "\n",
        "# Assemble Features\n",
        "assembler = VectorAssembler(inputCols=['Year', 'Month'], outputCol='features')\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "\n",
        "# Model Building\n",
        "lr = LinearRegression(featuresCol='scaledFeatures', labelCol='TotalQuantity')\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "# Simplified Hyperparameter Tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(lr.regParam, [0.1]) \\\n",
        "    .addGrid(lr.elasticNetParam, [0.5]) \\\n",
        "    .build()\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"TotalQuantity\")\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=2)  # Reduced folds for faster processing\n",
        "\n",
        "# Fit Model with Logs\n",
        "print(\"Starting model training with cross-validation...\")\n",
        "cvModel = crossval.fit(data)\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# Predict and Evaluate\n",
        "predictions = cvModel.transform(data)\n",
        "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
        "\n",
        "# Ensure Output Directory Exists\n",
        "task3_output_dir = '/content/output/task3'\n",
        "if os.path.exists(task3_output_dir):\n",
        "    if not os.path.isdir(task3_output_dir):\n",
        "        os.remove(task3_output_dir)  # Remove if it's a file\n",
        "os.makedirs(task3_output_dir, exist_ok=True)\n",
        "\n",
        "# Writing RMSE to a File\n",
        "rmse_file_path = os.path.join(task3_output_dir, 'rmse.txt')\n",
        "with open(rmse_file_path, 'w') as f:\n",
        "    f.write(f\"Root Mean Square Error (RMSE) on test data = {rmse}\\n\")\n",
        "\n",
        "# Writing MAE to a File\n",
        "mae_file_path = os.path.join(task3_output_dir, 'mae.txt')\n",
        "with open(mae_file_path, 'w') as f:\n",
        "    f.write(f\"Mean Absolute Error (MAE) on test data = {mae}\\n\")\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()\n",
        "\n",
        "print(f\"RMSE and MAE written to files:\\n{rmse_file_path}\\n{mae_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nycjSbeKqud9",
        "outputId": "877850c7-5895-41b7-b39c-2c7daf05ed45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training with cross-validation...\n",
            "Model training completed.\n",
            "RMSE and MAE written to files:\n",
            "/content/output/task3/rmse.txt\n",
            "/content/output/task3/mae.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, NGram, HashingTF, IDF, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import os\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sentiment Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define a UDF to concatenate arrays\n",
        "concat_arrays_udf = udf(lambda x, y: x + y, ArrayType(StringType()))\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/input/train.csv\"  # Update the path to your CSV file\n",
        "data = spark.read.csv(data_path, header=True, inferSchema=True).select(\"sentence\", \"main_image_url\")\n",
        "\n",
        "# Rename column for clarity\n",
        "data = data.withColumnRenamed(\"main_image_url\", \"product_id\")\n",
        "\n",
        "# Text Preprocessing\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
        "\n",
        "# Transform the data through the initial stages\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, ngram])\n",
        "transformed_data = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Combine the NGram and filtered columns into a single column for feature engineering\n",
        "transformed_data = transformed_data.withColumn(\"rawFeatures\", concat_arrays_udf(col(\"filtered\"), col(\"ngrams\")))\n",
        "\n",
        "# Feature Engineering using HashingTF and IDF\n",
        "hashingTF = HashingTF(inputCol=\"rawFeatures\", outputCol=\"features\", numFeatures=10000)\n",
        "featurized_data = hashingTF.transform(transformed_data)\n",
        "\n",
        "idf = IDF(inputCol=\"features\", outputCol=\"finalFeatures\")\n",
        "final_data = idf.fit(featurized_data).transform(featurized_data)\n",
        "\n",
        "# Prepare labels (assuming 'sentence' is not null when labeled)\n",
        "final_data = final_data.withColumn(\"label\", col(\"sentence\").isNotNull().cast(\"int\"))\n",
        "\n",
        "# Split data into training and test sets\n",
        "(trainingData, testData) = final_data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Logistic Regression Model\n",
        "lr = LogisticRegression(featuresCol='finalFeatures', labelCol='label')\n",
        "final_model = lr.fit(trainingData)\n",
        "\n",
        "# Predictions\n",
        "predictions = final_model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "# Ensure the task4 output directory exists\n",
        "task4_dir = \"/content/output/task4\"\n",
        "if os.path.exists(task4_dir):\n",
        "    if not os.path.isdir(task4_dir):\n",
        "        os.remove(task4_dir)  # Remove if it’s a file\n",
        "os.makedirs(task4_dir, exist_ok=True)\n",
        "\n",
        "# Write product sentiments to a subdirectory\n",
        "product_sentiments_path = os.path.join(task4_dir, \"product_sentiments\")\n",
        "product_sentiment = predictions.groupBy(\"product_id\").agg({\"prediction\": \"avg\"}).withColumnRenamed(\"avg(prediction)\", \"avg_sentiment\")\n",
        "product_sentiment.write.csv(product_sentiments_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# Save evaluation results in a text file\n",
        "results_file_path = os.path.join(task4_dir, \"evaluation_results.txt\")\n",
        "with open(results_file_path, \"w\") as file:\n",
        "    file.write(f\"Area Under ROC: {auc}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n",
        "print(f\"Results saved in {task4_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw0dNkUjrAml",
        "outputId": "33fc9952-7bf9-4d45-e76b-60f0b6e8cc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved in /content/output/task4\n"
          ]
        }
      ]
    }
  ]
}